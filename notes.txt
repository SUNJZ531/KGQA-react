2021/3/11: 修复了模型loss计算存在的问题，调整为l2范数并加入margin，计算公式为: score = margin - distance_r(h, t), loss = mean(-log(sigmoid(score)))
2021/3/12: （1）修复了KG_embed模型存在的grad问题，在第二阶段的训练中不再更新KG_embed模型的参数;
           （2）加入模型训练参数的保存，将日志写入文件；
           （3）修改relation预测方式，改进为softmax（scores）作为组合系数对KG_embed.rel_embeddings.weight进行线性组合（实验证明效果不如sigmoid）
2021/3/15: 在loss项中增加了negative部分，目前是naive的随机采样版本，实验证明训练效果有提升，后期可针对这一部分进一步优化
2021/3/16: （1）增加了对已训练模型的继续训练功能，可在表现较好的模型上进一步调优
           （2）增加了画图部分，对训练的过程进行可视化
2021/3/18: 进一步完善画图模块
2021/3/21：增加对其他embed模型的支持(complEx,实验表明效果不如rotatE(暂时))
2021/3/24: 使用聚类方法来对答案进行剪枝，生成候选答案集合，提高计算效率，***实验证明在提升效率的同时会损失一定的准确率***
2021/3/28：(1)改进question embed方法, 将question中的【head】统一替换为 xxx
           (2)统计训练过程中的gradient的范数，用于作为梯度裁剪的依据
2021/3/29: 增加对distmult的支持

2021/3/30: 改进relation预测方式，增加entity与relation的关联得分
2021/3/31: 更改模型结构，roberta部分参数不再进行微调（计算资源有限），而是将之作为embedding模块，精化后续的模块用于预测relation  ***取得新的最佳***

2021/4/2: 为了应对gradient vanishing，需要更新负采样策略（初步完成）, ***取得新的最佳***

2021/4/7: 增加了对attention的可视化，后续进一步调研attention主流方法；

接下来工作：（1）规划一下接下来的实验，有计划的搜集实验数据
          （2）增加对答案的rerank，看能不能提升表现
          （3）阅读attention相关论文
